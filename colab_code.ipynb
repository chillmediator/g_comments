{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!curl https://ollama.ai/install.sh | sh\n",
    "!pip install -q aiohttp pyngrok requests\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Set NVIDIA library path for GPU support\n",
    "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
    "\n",
    "# Basic error checking\n",
    "try:\n",
    "    !nvidia-smi\n",
    "    print(\"GPU is available\")\n",
    "except:\n",
    "    print(\"Warning: No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from asyncio import create_task\n",
    "\n",
    "async def run_process(cmd, capture_output=True):\n",
    "    print('>>> starting', *cmd)\n",
    "    if capture_output:\n",
    "        p = await asyncio.subprocess.create_subprocess_exec(\n",
    "            *cmd,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "        \n",
    "        async def pipe(lines):\n",
    "            async for line in lines:\n",
    "                print(line.strip().decode('utf-8'))\n",
    "        \n",
    "        await asyncio.gather(\n",
    "            pipe(p.stdout),\n",
    "            pipe(p.stderr),\n",
    "        )\n",
    "    else:\n",
    "        # For long-running processes, don't capture output\n",
    "        p = await asyncio.subprocess.create_subprocess_exec(*cmd)\n",
    "    return p\n",
    "\n",
    "async def wait_for_server(max_attempts=5):\n",
    "    \"\"\"Wait for Ollama server to be ready\"\"\"\n",
    "    for i in range(max_attempts):\n",
    "        try:\n",
    "            response = requests.get('http://localhost:11434')\n",
    "            if response.status_code == 200:\n",
    "                print(\"Server is ready!\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"Waiting for server... attempt {i+1}/{max_attempts}\")\n",
    "        await asyncio.sleep(2)\n",
    "    return False\n",
    "\n",
    "async def pull_model(model_name=\"mistral\", max_attempts=3):\n",
    "    \"\"\"Pull the specified model with retry logic\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                'http://localhost:11434/api/pull',\n",
    "                json={\"name\": model_name},\n",
    "                timeout=300\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Successfully pulled {model_name} model\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to pull model (attempt {attempt + 1}): {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error pulling model (attempt {attempt + 1}): {str(e)}\")\n",
    "        await asyncio.sleep(5)\n",
    "    return False\n",
    "\n",
    "async def verify_model():\n",
    "    \"\"\"Verify model is working with a test prompt\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'http://localhost:11434/api/generate',\n",
    "            json={\n",
    "                \"model\": \"mistral\",\n",
    "                \"prompt\": \"Say hello!\",\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"Model test successful!\")\n",
    "            print(\"Response:\", result.get('response', ''))\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Model test failed: {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "async def get_ngrok_url(max_attempts=5):\n",
    "    \"\"\"Get the public ngrok URL\"\"\"\n",
    "    for i in range(max_attempts):\n",
    "        try:\n",
    "            response = requests.get('http://localhost:4040/api/tunnels')\n",
    "            tunnels = response.json()['tunnels']\n",
    "            public_url = next(tunnel['public_url'] for tunnel in tunnels if 'ngrok' in tunnel['public_url'])\n",
    "            print(f\"\\nYour public API endpoint is: {public_url}\")\n",
    "            return public_url\n",
    "        except:\n",
    "            await asyncio.sleep(2)\n",
    "    print(\"Failed to get ngrok URL\")\n",
    "    return None\n",
    "\n",
    "async def main():\n",
    "    # Configure ngrok\n",
    "    NGROK_TOKEN = \"2de2r6zY6WYPiHQQtzbbag7Edyh_3oaAwPGgmmPhSbLPYeQb6\"\n",
    "    \n",
    "    print(\"Setting up services...\")\n",
    "    \n",
    "    # Configure ngrok\n",
    "    await run_process(['ngrok', 'config', 'add-authtoken', NGROK_TOKEN])\n",
    "    \n",
    "    # Start Ollama and ngrok as background tasks\n",
    "    ollama_process = await run_process(['ollama', 'serve'], capture_output=False)\n",
    "    ngrok_process = await run_process(\n",
    "        ['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    # Wait for server to be ready\n",
    "    if await wait_for_server():\n",
    "        # Pull and verify model\n",
    "        if await pull_model():\n",
    "            if await verify_model():\n",
    "                # Get and display ngrok URL\n",
    "                public_url = await get_ngrok_url()\n",
    "                if public_url:\n",
    "                    print(\"\\nSetup complete! Your Mistral model is ready to use.\")\n",
    "                    print(f\"Use this URL in your applications: {public_url}\")\n",
    "                    \n",
    "                    # Keep the processes running\n",
    "                    try:\n",
    "                        await asyncio.gather(\n",
    "                            ollama_process.wait(),\n",
    "                            ngrok_process.wait()\n",
    "                        )\n",
    "                    except KeyboardInterrupt:\n",
    "                        print(\"\\nShutting down services...\")\n",
    "                        ollama_process.terminate()\n",
    "                        ngrok_process.terminate()\n",
    "            else:\n",
    "                print(\"Failed to verify model\")\n",
    "        else:\n",
    "            print(\"Failed to pull model\")\n",
    "    else:\n",
    "        print(\"Server failed to start\")\n",
    "\n",
    "# Run the main async function\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
